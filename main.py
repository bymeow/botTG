from aiohttp import web
import asyncio
import logging
from aiogram import Bot, Dispatcher, types
from aiogram.filters import Command
import kb
from groq import Groq
import google.generativeai as genai
import os
import re
from memory import MemoryManager

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Gemini
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
model_flash = genai.GenerativeModel('gemini-1.5-flash')
model_pro = genai.GenerativeModel('gemini-1.5-pro')

bot = Bot(token=os.getenv("TG_TOKEN"))
dp = Dispatcher()

# --- –ò–ò-—Ç—é—Ç–æ—Ä ---
class SmartAITutor:
    def __init__(self, api_key):
        self.client = Groq(api_key=api_key)
        self.memory = MemoryManager()
        self.system_prompt = (
            "–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–µ (–ï–ì–≠). "
            "–ü–ò–®–ò –¢–û–õ–¨–ö–û –û–ë–´–ß–ù–´–ú –¢–ï–ö–°–¢–û–ú –ë–ï–ó –í–´–î–ï–õ–ï–ù–ò–ô. "
            "–ö–ê–¢–ï–ì–û–†–ò–ß–ï–°–ö–ò –ó–ê–ü–†–ï–©–ï–ù–û: –∂–∏—Ä–Ω—ã–π —à—Ä–∏—Ñ—Ç (**), –∫—É—Ä—Å–∏–≤ (*), —Ä–µ—à–µ—Ç–∫–∏ (#) –∏ –ª—é–±—ã–µ —Ç–µ–≥–∏. "
            "–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –ò–°–ü–û–õ–¨–ó–£–ô –≠–ú–û–î–ó–ò: üìö, üí°, ‚úÖ, ‚ùå, üéØ. "
            "–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ, —Ä–∞–∑–¥–µ–ª—è—è –º—ã—Å–ª–∏ –ø—Ä–æ—Å—Ç–æ –Ω–æ–≤—ã–º–∏ —Å—Ç—Ä–æ–∫–∞–º–∏."
        )

    def clean_response(self, text: str) -> str:
        """–£–¥–∞–ª—è–µ—Ç —Å–∏–º–≤–æ–ª—ã —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –µ—Å–ª–∏ –ò–ò –∏—Ö –≤—Å–µ –∂–µ –¥–æ–±–∞–≤–∏–ª"""
        text = re.sub(r'<[^>]+>', '', text)  # –£–¥–∞–ª—è–µ–º HTML
        text = re.sub(r'#{1,6}\s+', '', text)  # –£–¥–∞–ª—è–µ–º —Ä–µ—à—ë—Ç–∫–∏
        text = text.replace('**', '').replace('*', '') # –£–¥–∞–ª—è–µ–º Markdown –∂–∏—Ä–Ω—ã–π/–∫—É—Ä—Å–∏–≤
        return text.strip()

    async def get_ai_response(self, user_id: int, message: str):
        uid = str(user_id)
        self.memory.add_message_to_history(uid, "user", message)
        history = self.memory.get_recent_context(uid)
        messages = [{"role": "system", "content": self.system_prompt}] + history

        try:
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(None, lambda: self.client.chat.completions.create(
                model="llama-3.3-70b-versatile", messages=messages, temperature=0.7
            ))
            ai_text = self.clean_response(response.choices[0].message.content)
            self.memory.add_message_to_history(uid, "assistant", ai_text)
            return ai_text
        except Exception as e:
            logging.error(f"Groq –æ—à–∏–±–∫–∞: {e}")
            return "‚ö†Ô∏è –û—à–∏–±–∫–∞ Groq. –ü–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑!"

tutor = SmartAITutor(api_key=os.getenv("GROQ_KEY"))

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ Gemini ---
async def get_gemini_response(prompt, model_type="flash"):
    try:
        selected_model = model_pro if model_type == "pro" else model_flash
        instruction = (
            "–¢—ã —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–µ. –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ (üìö, ‚úÖ, üí°). "
            "–ü–ò–®–ò –¢–û–õ–¨–ö–û –û–ë–´–ß–ù–´–ú –¢–ï–ö–°–¢–û–ú. –ó–ê–ü–†–ï–©–ï–ù –ñ–ò–†–ù–´–ô –®–†–ò–§–¢ –ò –ö–£–†–°–ò–í. "
            f"–í–æ–ø—Ä–æ—Å: {prompt}"
        )
        # –í—ã–∑–æ–≤ Gemini —á–µ—Ä–µ–∑ –ø–æ—Ç–æ–∫
        response = await asyncio.to_thread(selected_model.generate_content, instruction)
        
        if not response.text:
            return "‚ö†Ô∏è Gemini –ø—Ä–∏—Å–ª–∞–ª–∞ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç."
            
        return tutor.clean_response(response.text)
    except Exception as e:
        logging.error(f"Gemini –æ—à–∏–±–∫–∞: {e}")
        return "‚ö†Ô∏è –û—à–∏–±–∫–∞ Gemini (–ª–∏–º–∏—Ç 2 –∑–∞–ø/–º–∏–Ω –¥–ª—è Pro). –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ!"

# --- –•–µ–Ω–¥–ª–µ—Ä—ã ---
@dp.message(Command("start"))
async def start_cmd(msg: types.Message):
    await msg.answer(
        "üëã –ü—Ä–∏–≤–µ—Ç! –Ø —Ç–≤–æ–π –ò–ò-—Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–µ!\n\n"
        "üéì –ü–æ–º–æ–≥—É —Å –ï–ì–≠ | üí° –û–±—ä—è—Å–Ω—é –ø—Ä–æ—Å—Ç–æ | üìù –†–∞–∑–±–µ—Ä—É –∑–∞–¥–∞—á–∏\n\n"
        "‚¨áÔ∏è –ò—Å–ø–æ–ª—å–∑—É–π –∫–Ω–æ–ø–∫–∏ –º–µ–Ω—é",
        reply_markup=kb.main_menu()
    )

@dp.message(lambda m: m.text == "ü§ñ –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏")
async def choose_model(msg: types.Message):
    data = tutor.memory.load_user_data(msg.from_user.id)
    current = data.get("current_model", "groq")
    models = {"groq": "Groq", "flash": "Flash", "pro": "Pro"}
    await msg.answer(
        f"–°–µ–π—á–∞—Å: {models.get(current)}\n–í—ã–±–µ—Ä–∏ –º–æ–¥–µ–ª—å:",
        reply_markup=kb.model_selector()
    )

@dp.callback_query(lambda c: c.data.startswith('set_model_'))
async def set_model(cq: types.CallbackQuery):
    model = cq.data.split('_')[2]
    data = tutor.memory.load_user_data(cq.from_user.id)
    data["current_model"] = model
    tutor.memory.save_user_data(cq.from_user.id, data)
    await cq.answer(f"–ú–æ–¥–µ–ª—å: {model.upper()}")
    await cq.message.edit_text(f"‚úÖ –ú–æ–¥–µ–ª—å –∏–∑–º–µ–Ω–µ–Ω–∞ –Ω–∞: {model.upper()}")

@dp.message(lambda m: m.text == "üìö –¢–µ–º—ã –ï–ì–≠")
async def show_topics(msg: types.Message):
    await msg.answer(
        "üìö –¢–µ–º—ã –ï–ì–≠:\n\n"
        "1. –°–∏—Å—Ç–µ–º—ã —Å—á–∏—Å–ª–µ–Ω–∏—è üî¢\n"
        "2. –ê–ª–≥–µ–±—Ä–∞ –ª–æ–≥–∏–∫–∏ üßÆ\n"
        "3. –ê–ª–≥–æ—Ä–∏—Ç–º—ã üîÑ\n"
        "4. –ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ üíª\n"
        "5. –°–µ—Ç–∏ üåê\n\n"
        "üí° –ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏ –≤–æ–ø—Ä–æ—Å!"
    )

@dp.message(lambda m: m.text == "üîÑ –ù–æ–≤—ã–π –¥–∏–∞–ª–æ–≥")
async def reset_history(msg: types.Message):
    data = tutor.memory.load_user_data(msg.from_user.id)
    data["conversation_history"] = []
    tutor.memory.save_user_data(msg.from_user.id, data)
    await msg.answer("üßπ –ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞! ‚ú®")

@dp.message(lambda m: m.text == "üìâ –ú–æ–π –ø—Ä–æ–≥—Ä–µ—Å—Å")
async def show_progress(msg: types.Message):
    data = tutor.memory.load_user_data(msg.from_user.id)
    mistakes = data.get("learning_progress", {}).get("common_mistakes", [])
    text = "üìä –ü—Ä–æ–≥—Ä–µ—Å—Å:\n‚úÖ –û—à–∏–±–æ–∫ –Ω–µ—Ç" if not mistakes else f"üìä –û—à–∏–±–∫–∏:\n{mistakes}"
    await msg.answer(text)

# --- –û—Å–Ω–æ–≤–Ω–æ–π —á–∞—Ç (–ë–ï–ó MARKDOWN) ---
@dp.message()
async def chat_handler(msg: types.Message):
    await bot.send_chat_action(msg.chat.id, "typing")
    data = tutor.memory.load_user_data(msg.from_user.id)
    model = data.get("current_model", "groq")

    try:
        if model == "pro":
            answer = await get_gemini_response(msg.text, "pro")
        elif model == "flash":
            answer = await get_gemini_response(msg.text, "flash")
        else:
            answer = await tutor.get_ai_response(msg.from_user.id, msg.text)
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ parse_mode
        await msg.answer(answer)
        
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —á–∞—Ç–∞: {e}")
        await msg.answer("‚ö†Ô∏è –û—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π —Å–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –∏–ª–∏ –Ω–∞—á–∞—Ç—å –Ω–æ–≤—ã–π –¥–∏–∞–ª–æ–≥.")

# --- –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ –¥–ª—è Koyeb ---
async def main():
    app = web.Application()
    app.router.add_get('/', lambda r: web.Response(text="Bot OK"))
    runner = web.AppRunner(app)
    await runner.setup()
    await web.TCPSite(runner, '0.0.0.0', int(os.getenv('PORT', 8000))).start()
    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())
