from aiohttp import web
import asyncio
import logging
from aiogram import Bot, Dispatcher, types
from aiogram.filters import Command
import kb
from groq import Groq
import google.generativeai as genai
import os
import styles
from memory import MemoryManager
import re

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
model_flash = genai.GenerativeModel('gemini-1.5-flash')
model_pro = genai.GenerativeModel('gemini-1.5-pro')

bot = Bot(token=os.getenv("TG_TOKEN"))
dp = Dispatcher()

# --- –ò–ò-—Ç—é—Ç–æ—Ä ---
class SmartAITutor:
    def __init__(self, api_key):
        self.client = Groq(api_key=api_key)
        self.memory = MemoryManager()
        self.system_prompt = (
            "–¢—ã ‚Äî —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–µ (–ï–ì–≠) üíª\n"
            "–ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û Markdown: *–∫—É—Ä—Å–∏–≤*, **–∂–∏—Ä–Ω—ã–π**, `–∫–æ–¥`\n"
            "–ó–ê–ü–†–ï–©–ï–ù–û: HTML, —Ä–µ—à—ë—Ç–∫–∏ (#)\n"
            "–≠–º–æ–¥–∑–∏: üìö —Ç–µ–æ—Ä–∏—è | üí° –ø–æ–¥—Å–∫–∞–∑–∫–∏ | ‚úÖ –ø—Ä–∞–≤–∏–ª—å–Ω–æ | ‚ùå –æ—à–∏–±–∫–∞ | üéØ –≤–∞–∂–Ω–æ–µ\n"
            "–ù–µ –¥–∞–≤–∞–π –≥–æ—Ç–æ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ ‚Äî –∑–∞–¥–∞–≤–∞–π –Ω–∞–≤–æ–¥—è—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã üåü"
        )

    def clean_response(self, text: str) -> str:
        """–£–¥–∞–ª—è–µ—Ç –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã"""
        text = re.sub(r'<[^>]+>', '', text)  # –£–¥–∞–ª—è–µ–º HTML
        text = re.sub(r'#{1,6}\s+', '', text)  # –£–¥–∞–ª—è–µ–º —Ä–µ—à—ë—Ç–∫–∏
        return text.strip()

    async def get_ai_response(self, user_id: int, message: str):
        uid = str(user_id)
        self.memory.add_message_to_history(uid, "user", message)
        history = self.memory.get_recent_context(uid)
        messages = [{"role": "system", "content": self.system_prompt}] + history

        try:
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(None, lambda: self.client.chat.completions.create(
                model="llama-3.3-70b-versatile", messages=messages, temperature=0.7, max_tokens=2048
            ))
            ai_text = self.clean_response(response.choices[0].message.content)
            self.memory.add_message_to_history(uid, "assistant", ai_text)
            return ai_text
        except Exception as e:
            logging.error(f"Groq –æ—à–∏–±–∫–∞: {e}")
            return "‚ö†Ô∏è –û—à–∏–±–∫–∞ –ò–ò. –ü–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑!"

tutor = SmartAITutor(api_key=os.getenv("GROQ_KEY"))

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---
async def get_gemini_response(prompt, model_type="flash"):
    try:
        model = model_pro if model_type == "pro" else model_flash
        response = await asyncio.to_thread(model.generate_content, prompt)
        return tutor.clean_response(response.text)  # –ß–∏—Å—Ç–∏–º –æ—Ç–≤–µ—Ç Gemini
    except Exception as e:
        logging.error(f"Gemini –æ—à–∏–±–∫–∞: {e}")
        return "‚ö†Ô∏è –û—à–∏–±–∫–∞ Gemini. –ü–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑!"

# --- –•–µ–Ω–¥–ª–µ—Ä—ã ---
@dp.message(Command("start"))
async def start_cmd(msg: types.Message):
    await msg.answer(
        "üëã **–ü—Ä–∏–≤–µ—Ç! –Ø —Ç–≤–æ–π –ò–ò-—Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–µ!**\n\n"
        "üéì –ü–æ–º–æ–≥—É —Å –ï–ì–≠ | üí° –û–±—ä—è—Å–Ω—é –ø—Ä–æ—Å—Ç–æ | üìù –†–∞–∑–±–µ—Ä—É –∑–∞–¥–∞—á–∏\n\n"
        "‚¨áÔ∏è *–ò—Å–ø–æ–ª—å–∑—É–π –∫–Ω–æ–ø–∫–∏ –º–µ–Ω—é*",
        reply_markup=kb.main_menu(), parse_mode="Markdown"
    )

@dp.message(lambda m: m.text == "ü§ñ –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏")
async def choose_model(msg: types.Message):
    data = tutor.memory.load_user_data(msg.from_user.id)
    current = data.get("current_model", "groq")
    models = {"groq": "üöÄ Groq", "flash": "‚ö° Flash", "pro": "üéØ Pro"}
    await msg.answer(
        f"–°–µ–π—á–∞—Å: **{models.get(current)}**\n–í—ã–±–µ—Ä–∏ –º–æ–¥–µ–ª—å:",
        reply_markup=kb.model_selector(), parse_mode="Markdown"
    )

@dp.callback_query(lambda c: c.data.startswith('set_model_'))
async def set_model(cq: types.CallbackQuery):
    model = cq.data.split('_')[2]
    data = tutor.memory.load_user_data(cq.from_user.id)
    data["current_model"] = model
    tutor.memory.save_user_data(cq.from_user.id, data)
    emoji = {"groq": "üöÄ", "flash": "‚ö°", "pro": "üéØ"}[model]
    await cq.answer(f"–ú–æ–¥–µ–ª—å: {model.upper()} {emoji}")
    await cq.message.edit_text(f"‚úÖ –ú–æ–¥–µ–ª—å: **{model.upper()}** {emoji}", parse_mode="Markdown")

@dp.message(lambda m: m.text == "üìö –¢–µ–º—ã –ï–ì–≠")
async def show_topics(msg: types.Message):
    await msg.answer(
        "üìö **–¢–µ–º—ã –ï–ì–≠:**\n\n"
        "1Ô∏è‚É£ –°–∏—Å—Ç–µ–º—ã —Å—á–∏—Å–ª–µ–Ω–∏—è üî¢\n"
        "2Ô∏è‚É£ –ê–ª–≥–µ–±—Ä–∞ –ª–æ–≥–∏–∫–∏ üßÆ\n"
        "3Ô∏è‚É£ –ê–ª–≥–æ—Ä–∏—Ç–º—ã üîÑ\n"
        "4Ô∏è‚É£ –ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ üíª\n"
        "5Ô∏è‚É£ –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ üìä\n"
        "6Ô∏è‚É£ –ë–î –∏ Excel üìà\n"
        "7Ô∏è‚É£ –°–µ—Ç–∏ üåê\n\n"
        "üí° *–ó–∞–¥–∞–π –≤–æ–ø—Ä–æ—Å –ø–æ —Ç–µ–º–µ!*",
        parse_mode="Markdown"
    )

@dp.message(lambda m: m.text == "üîÑ –ù–æ–≤—ã–π –¥–∏–∞–ª–æ–≥")
async def reset_history(msg: types.Message):
    data = tutor.memory.load_user_data(msg.from_user.id)
    data["conversation_history"] = []
    tutor.memory.save_user_data(msg.from_user.id, data)
    await msg.answer("üßπ **–ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞!** ‚ú®", parse_mode="Markdown")

@dp.message(lambda m: m.text == "üìâ –ú–æ–π –ø—Ä–æ–≥—Ä–µ—Å—Å")
async def show_progress(msg: types.Message):
    data = tutor.memory.load_user_data(msg.from_user.id)
    mistakes = data.get("learning_progress", {}).get("common_mistakes", [])
    count = len(data.get("conversation_history", []))
    
    if not mistakes:
        text = f"üìä **–ü—Ä–æ–≥—Ä–µ—Å—Å:**\nüí¨ –°–æ–æ–±—â–µ–Ω–∏–π: {count}\n‚úÖ –û—à–∏–±–æ–∫ –Ω–µ—Ç\nüåü –û—Ç–ª–∏—á–Ω–æ! üí™"
    else:
        m_list = "\n".join([f"‚Ä¢ {m}" for m in mistakes[:5]])
        text = f"üìä **–ü—Ä–æ–≥—Ä–µ—Å—Å:**\nüí¨ –°–æ–æ–±—â–µ–Ω–∏–π: {count}\n‚ö†Ô∏è –û—à–∏–±–∫–∏:\n{m_list}\nüí° –†–∞–∑–±–µ—Ä—ë–º –∏—Ö!"
    
    await msg.answer(text, parse_mode="Markdown")

# --- –û—Å–Ω–æ–≤–Ω–æ–π —á–∞—Ç ---
@dp.message()
async def chat_handler(msg: types.Message):
    await bot.send_chat_action(msg.chat.id, "typing")
    data = tutor.memory.load_user_data(msg.from_user.id)
    model = data.get("current_model", "groq")

    try:
        if model == "pro":
            answer = await get_gemini_response(msg.text, "pro")
        elif model == "flash":
            answer = await get_gemini_response(msg.text, "flash")
        else:
            answer = await tutor.get_ai_response(msg.from_user.id, msg.text)
        
        if answer:
            pretty = styles.format_bot_response(answer)
            await msg.answer(pretty, parse_mode="Markdown")
        else:
            await msg.answer("‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç. –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –≤–æ–ø—Ä–æ—Å.")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —á–∞—Ç–∞: {e}")
        await msg.answer("‚ö†Ô∏è –û—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π:\n‚Ä¢ –î—Ä—É–≥—É—é –º–æ–¥–µ–ª—å ü§ñ\n‚Ä¢ –ù–æ–≤—ã–π –¥–∏–∞–ª–æ–≥ üîÑ")

# --- –ó–∞–ø—É—Å–∫ ---
async def main():
    app = web.Application()
    app.router.add_get('/', lambda r: web.Response(text="Bot OK ü§ñ"))
    runner = web.AppRunner(app)
    await runner.setup()
    await web.TCPSite(runner, '0.0.0.0', int(os.getenv('PORT', 8000))).start()
    logging.info("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω!")
    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())
