from aiohttp import web
import asyncio
import logging
from aiogram import Bot, Dispatcher, types
from aiogram.filters import Command
import kb
from groq import Groq
import google.generativeai as genai
import os
import re
from memory import MemoryManager

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Gemini
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
model_flash = genai.GenerativeModel('gemini-1.5-flash')
model_pro = genai.GenerativeModel('gemini-1.5-pro')

bot = Bot(token=os.getenv("TG_TOKEN"))
dp = Dispatcher()

# --- –ö–ª–∞—Å—Å —Ç—é—Ç–æ—Ä–∞ –ë–ï–ó —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è ---
class SmartAITutor:
    def __init__(self, api_key):
        self.client = Groq(api_key=api_key)
        self.memory = MemoryManager()
        # –ü—Ä—è–º–æ –∑–∞–ø—Ä–µ—â–∞–µ–º –ª—é–±–æ–π –∂–∏—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        self.system_prompt = (
            "–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–µ (–ï–ì–≠). "
            "–ü–ò–®–ò –¢–û–õ–¨–ö–û –û–ë–´–ß–ù–´–ú –¢–ï–ö–°–¢–û–ú. "
            "–ö–ê–¢–ï–ì–û–†–ò–ß–ï–°–ö–ò –ó–ê–ü–†–ï–©–ï–ù–û –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å: **, __, <b>, <i>, #. "
            "–ò–°–ü–û–õ–¨–ó–£–ô –≠–ú–û–î–ó–ò –¥–ª—è –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è: üìö, üí°, ‚úÖ, ‚ùå, üéØ. "
            "–†–∞–∑–¥–µ–ª—è–π –º—ã—Å–ª–∏ –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–Ω–æ—Å–æ–º —Å—Ç—Ä–æ–∫–∏."
        )

    def clean_all(self, text: str) -> str:
        """–ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –æ—Ç –ª—é–±–æ–≥–æ –º—É—Å–æ—Ä–∞"""
        text = re.sub(r'<[^>]+>', '', text)  # –£–¥–∞–ª—è–µ—Ç <b>, </b> –∏ —Ç.–¥.
        text = text.replace('**', '').replace('*', '').replace('`', '')
        text = re.sub(r'#{1,6}\s+', '', text)
        return text.strip()

    async def get_ai_response(self, user_id: int, message: str):
        uid = str(user_id)
        self.memory.add_message_to_history(uid, "user", message)
        history = self.memory.get_recent_context(uid)
        messages = [{"role": "system", "content": self.system_prompt}] + history

        try:
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(None, lambda: self.client.chat.completions.create(
                model="llama-3.3-70b-versatile", messages=messages, temperature=0.7
            ))
            ai_text = self.clean_all(response.choices[0].message.content)
            self.memory.add_message_to_history(uid, "assistant", ai_text)
            return ai_text
        except Exception as e:
            logging.error(f"Groq error: {e}")
            return "‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–≤—è–∑–∏. –ü–æ–ø—Ä–æ–±—É–π –µ—â–µ —Ä–∞–∑."

tutor = SmartAITutor(api_key=os.getenv("GROQ_KEY"))

# --- –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Gemini ---
async def get_gemini_response(prompt, model_type="flash"):
    try:
        selected_model = model_pro if model_type == "pro" else model_flash
        instruction = (
            "–¢—ã —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–µ. –ò—Å–ø–æ–ª—å–∑—É–π —Å–º–∞–π–ª–∏–∫–∏. "
            "–ù–ï –ò–°–ü–û–õ–¨–ó–£–ô –∂–∏—Ä–Ω—ã–π —à—Ä–∏—Ñ—Ç –∏–ª–∏ HTML. –ü–∏—à–∏ –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç. "
            f"–í–æ–ø—Ä–æ—Å: {prompt}"
        )
        response = await asyncio.to_thread(selected_model.generate_content, instruction)
        return tutor.clean_all(response.text)
    except Exception as e:
        logging.error(f"Gemini error: {e}")
        return "‚ö†Ô∏è –û—à–∏–±–∫–∞ Gemini. –ü–æ–ø—Ä–æ–±—É–π —á–µ—Ä–µ–∑ –º–∏–Ω—É—Ç—É."

# --- –•–µ–Ω–¥–ª–µ—Ä—ã (–£–±—Ä–∞–ª–∏ parse_mode –≤–µ–∑–¥–µ) ---
@dp.message(Command("start"))
async def start_cmd(msg: types.Message):
    await msg.answer(
        "üëã –ü—Ä–∏–≤–µ—Ç! –Ø —Ç–≤–æ–π –ò–ò-—Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–µ!\n\n"
        "üéì –ü–æ–º–æ–≥—É —Å –ï–ì–≠ | üí° –û–±—ä—è—Å–Ω—é –ø—Ä–æ—Å—Ç–æ\n\n"
        "‚¨áÔ∏è –ò—Å–ø–æ–ª—å–∑—É–π –∫–Ω–æ–ø–∫–∏ –Ω–∏–∂–µ",
        reply_markup=kb.main_menu()
    )

@dp.message(lambda m: m.text == "ü§ñ –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏")
async def choose_model(msg: types.Message):
    await msg.answer("–í—ã–±–µ—Ä–∏ –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:", reply_markup=kb.model_selector())

@dp.callback_query(lambda c: c.data.startswith('set_model_'))
async def set_model(cq: types.CallbackQuery):
    model = cq.data.split('_')[2]
    data = tutor.memory.load_user_data(cq.from_user.id)
    data["current_model"] = model
    tutor.memory.save_user_data(cq.from_user.id, data)
    await cq.answer(f"–í—ã–±—Ä–∞–Ω–æ: {model.upper()}")
    await cq.message.edit_text(f"‚úÖ –¢–µ–ø–µ—Ä—å —è –∏—Å–ø–æ–ª—å–∑—É—é –º–æ–¥–µ–ª—å: {model.upper()}")

@dp.message(lambda m: m.text == "üîÑ –ù–æ–≤—ã–π –¥–∏–∞–ª–æ–≥")
async def reset_history(msg: types.Message):
    data = tutor.memory.load_user_data(msg.from_user.id)
    data["conversation_history"] = []
    tutor.memory.save_user_data(msg.from_user.id, data)
    await msg.answer("üßπ –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ –æ—á–∏—â–µ–Ω–∞!")

# --- –ì–õ–ê–í–ù–´–ô –ß–ê–¢ ---
@dp.message()
async def chat_handler(msg: types.Message):
    await bot.send_chat_action(msg.chat.id, "typing")
    data = tutor.memory.load_user_data(msg.from_user.id)
    model = data.get("current_model", "groq")

    try:
        if model == "pro":
            answer = await get_gemini_response(msg.text, "pro")
        elif model == "flash":
            answer = await get_gemini_response(msg.text, "flash")
        else:
            answer = await tutor.get_ai_response(msg.from_user.id, msg.text)
        
        # –°–ê–ú–û–ï –í–ê–ñ–ù–û–ï: –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –±–µ–∑ parse_mode
        await msg.answer(answer)
        
    except Exception as e:
        logging.error(f"Chat error: {e}")
        await msg.answer("‚ö†Ô∏è –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π —Å–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å.")

async def main():
    app = web.Application()
    app.router.add_get('/', lambda r: web.Response(text="Bot is running!"))
    runner = web.AppRunner(app)
    await runner.setup()
    await web.TCPSite(runner, '0.0.0.0', int(os.getenv('PORT', 8000))).start()
    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())
