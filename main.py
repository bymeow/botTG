from aiohttp import web
import asyncio
import logging
from aiogram import Bot, Dispatcher, types
from aiogram.filters import Command
import kb  # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à —Ñ–∞–π–ª —Å –∫–Ω–æ–ø–∫–∞–º–∏
from groq import Groq
import google.generativeai as genai
import os
import styles
from memory import MemoryManager
import re

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ---
logging.basicConfig(level=logging.INFO)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Gemini
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
model_flash = genai.GenerativeModel('gemini-1.5-flash')
model_pro = genai.GenerativeModel('gemini-1.5-pro')

TOKEN = os.getenv("TG_TOKEN")
GROQ_API_KEY = os.getenv("GROQ_KEY")

bot = Bot(token=TOKEN)
dp = Dispatcher()


# --- –ö–ª–∞—Å—Å  –ò–ò-—Ç—é—Ç–æ—Ä–∞ (—Å –≤–æ–∑–≤—Ä–∞—â–µ–Ω–Ω—ã–º–∏ —ç–º–æ–¥–∑–∏) ---
class SmartAITutor:
    def __init__(self, api_key):
        self.client = Groq(api_key=api_key)
        self.memory = MemoryManager()
        self.system_prompt = (
            "–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–µ (–ï–ì–≠). "
            "–í–ê–ñ–ù–û: –ù–ò–ö–û–ì–î–ê –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å–∏–º–≤–æ–ª—ã —Ä–µ—à—ë—Ç–∫–∏ (#, ##, ###) –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤. "
            "–ù–ò–ö–û–ì–î–ê –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π HTML-—Ç–µ–≥–∏ (<b>, </b>, <i>, </i> –∏ –ª—é–±—ã–µ –¥—Ä—É–≥–∏–µ). "
            "–î–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –∑–≤—ë–∑–¥–æ—á–∫–∏: *—Ç–µ–∫—Å—Ç* –¥–ª—è –∫—É—Ä—Å–∏–≤–∞, **—Ç–µ–∫—Å—Ç** –¥–ª—è –∂–∏—Ä–Ω–æ–≥–æ. "
            "–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–π —Å–ø–∏—Å–∫–∏ —Å –¥–µ—Ñ–∏—Å–∞–º–∏ (-) –∏–ª–∏ —Ü–∏—Ñ—Ä–∞–º–∏. "
            "–ê–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏: üìö —Ç–µ–æ—Ä–∏—è, üí° –ø–æ–¥—Å–∫–∞–∑–∫–∏, "
            "‚úÖ –ø—Ä–∞–≤–∏–ª—å–Ω–æ, ‚ùå –æ—à–∏–±–∫–∞, ‚ùì –≤–æ–ø—Ä–æ—Å—ã, üéØ –≤–∞–∂–Ω–æ–µ, üìù –ø—Ä–∏–º–µ—Ä—ã, üî¢ —Ñ–æ—Ä–º—É–ª—ã, ‚ö†Ô∏è —á–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏. "
            "–ù–µ –¥–∞–≤–∞–π —Ä–µ—à–µ–Ω–∏–µ —Å—Ä–∞–∑—É, –∑–∞–¥–∞–≤–∞–π –Ω–∞–≤–æ–¥—è—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã."
        )

    def clean_response(self, text: str) -> str:
        """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏–∑ –æ—Ç–≤–µ—Ç–∞ –ò–ò"""
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'^#{1,6}\s+', '', text, flags=re.MULTILINE)
        text = re.sub(r'#{1,6}\s+', '', text)
        return text

    async def get_ai_response(self, user_id: int, message: str):
        uid = str(user_id)
        self.memory.add_message_to_history(uid, "user", message)
        history = self.memory.get_recent_context(uid)
        messages = [{"role": "system", "content": self.system_prompt}] + history

        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(None, lambda: self.client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=messages,
            temperature=0.7
        ))
        
        ai_text = self.clean_response(response.choices[0].message.content)
        self.memory.add_message_to_history(uid, "assistant", ai_text)
        return ai_text

tutor = SmartAITutor(api_key=GROQ_API_KEY)

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---
async def get_gemini_response(prompt, model_type="flash"):
    selected_model = model_pro if model_type == "pro" else model_flash
    response = await asyncio.to_thread(selected_model.generate_content, prompt)
    return response.text

# --- –•–µ–Ω–¥–ª–µ—Ä—ã –∫–æ–º–∞–Ω–¥ –∏ –º–µ–Ω—é ---
@dp.message(Command("start"))
async def start_cmd(message: types.Message):
    welcome_text = (
        "üëã <b>–ü—Ä–∏–≤–µ—Ç! –Ø —Ç–≤–æ–π –ò–ò-—Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–µ!</b>\n\n"
        "üìö –ì–æ—Ç–æ–≤ –ø–æ–º–æ—á—å —Å –ø–æ–¥–≥–æ—Ç–æ–≤–∫–æ–π –∫ –ï–ì–≠\n‚¨áÔ∏è <i>–ò—Å–ø–æ–ª—å–∑—É–π –∫–Ω–æ–ø–∫–∏ –Ω–∏–∂–µ:</i>"
    )
    await message.answer(welcome_text, reply_markup=kb.main_menu(), parse_mode="HTML")

@dp.message(lambda message: message.text == "ü§ñ –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏")
async def choose_model(message: types.Message):
    user_id = message.from_user.id
    data = tutor.memory.load_user_data(user_id)
    current_model = data.get("current_model", "groq")
    await message.answer(
        f"–°–µ–π—á–∞—Å –∞–∫—Ç–∏–≤–Ω–∞ –º–æ–¥–µ–ª—å: <b>{current_model.upper()}</b>\n–í—ã–±–µ—Ä–∏ –º–æ–∑–≥–∏:",
        reply_markup=kb.model_selector(),
        parse_mode="HTML"
    )

@dp.callback_query(lambda c: c.data.startswith('set_model_'))
async def process_model_selection(callback_query: types.CallbackQuery):
    model_name = callback_query.data.split('_')[2]
    user_id = callback_query.from_user.id
    data = tutor.memory.load_user_data(user_id)
    data["current_model"] = model_name
    tutor.memory.save_user_data(user_id, data)
    await callback_query.answer(f"–ú–æ–¥–µ–ª—å –∏–∑–º–µ–Ω–µ–Ω–∞ –Ω–∞ {model_name.upper()}!")
    await callback_query.message.edit_text(f"‚úÖ –¢–µ–ø–µ—Ä—å —è –∏—Å–ø–æ–ª—å–∑—É—é: <b>{model_name.upper()}</b>", parse_mode="HTML")

@dp.message(lambda message: message.text == "üìö –¢–µ–º—ã –ï–ì–≠")
async def show_topics(message: types.Message):
    topics_text = "<b>–ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã –ï–ì–≠:</b>\n1. –õ–æ–≥–∏–∫–∞\n2. –ê–ª–≥–æ—Ä–∏—Ç–º—ã\n3. Python\n4. –°–µ—Ç–∏"
    await message.answer(topics_text, parse_mode="HTML")

@dp.message(lambda message: message.text == "üîÑ –ù–æ–≤—ã–π –¥–∏–∞–ª–æ–≥")
async def reset_history(message: types.Message):
    user_id = message.from_user.id
    data = tutor.memory.load_user_data(user_id)
    data["conversation_history"] = []
    tutor.memory.save_user_data(user_id, data)
    await message.answer("üßº <b>–ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞!</b>", parse_mode="HTML")

@dp.message(lambda message: message.text == "üìâ –ú–æ–π –ø—Ä–æ–≥—Ä–µ—Å—Å")
async def show_progress(message: types.Message):
    user_id = message.from_user.id
    data = tutor.memory.load_user_data(user_id)
    mistakes = data.get("learning_progress", {}).get("common_mistakes", [])
    response = "–¢–≤–æ–π –ø—Ä–æ–≥—Ä–µ—Å—Å —á–∏—Å—Ç!" if not mistakes else f"–¢–≤–æ–∏ –æ—à–∏–±–∫–∏: {', '.join(mistakes)}"
    await message.answer(f"<b>{response}</b>", parse_mode="HTML")

# --- –û—Å–Ω–æ–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —á–∞—Ç–∞ ---
@dp.message()
async def chat_handler(message: types.Message):
    await bot.send_chat_action(message.chat.id, "typing")
    user_id = message.from_user.id
    data = tutor.memory.load_user_data(user_id)
    current_model = data.get("current_model", "groq")
    raw_answer = ""

    try:
        if current_model == "pro":
            raw_answer = await get_gemini_response(message.text, model_type="pro")
        elif current_model == "flash":
            raw_answer = await get_gemini_response(message.text, model_type="flash")
        else:
            raw_answer = await tutor.get_ai_response(user_id, message.text)
            
        pretty_answer = styles.format_bot_response(raw_answer)
        await message.answer(pretty_answer, parse_mode="Markdown")
        
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞: {e}")
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç –±–µ–∑ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –µ—Å–ª–∏ Markdown –ø–æ–¥–≤–µ–ª
            await message.answer(raw_answer if raw_answer else "‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–≤—è–∑–∏ —Å –ò–ò.")
        except:
            await message.answer("‚ö†Ô∏è –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ.")

# --- –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ –¥–ª—è Koyeb ---
async def main():
    app = web.Application()
    app.router.add_get('/', lambda r: web.Response(text="OK"))
    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, '0.0.0.0', 8000)
    await site.start()
    
    logging.info("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω!")
    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())
